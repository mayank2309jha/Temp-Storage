{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6d495b-719d-4bfe-a15c-4e6599a75f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_URLS = {\n",
    "    \"india\": \"https://indianexpress.com/section/india/?ref=l1_section\",\n",
    "    \"entertainment\": \"https://indianexpress.com/section/entertainment/?ref=l1_section\",\n",
    "    \"politics\": \"https://indianexpress.com/politics/?ref=l1_section\",\n",
    "    \"sports\": \"https://indianexpress.com/section/sports/?ref=l1_section\",\n",
    "    \"world\": \"https://indianexpress.com/section/world/?ref=l1_section\",\n",
    "    \"business\": \"https://indianexpress.com/section/business/?ref=l1_section\",\n",
    "    \"technology\": \"https://indianexpress.com/section/technology/?ref=l1_section\"\n",
    "}\n",
    "JSON_NAMES = {\n",
    "    \"india\": \"india_news.json\",\n",
    "    \"entertainment\": \"entertainment_news.json\",\n",
    "    \"politics\": \"politics_news.json\",  # NEW JSON WILL BE CREATED\n",
    "    \"sports\": \"sports_news.json\",\n",
    "    \"world\": \"world_news.json\",\n",
    "    \"business\": \"business_news.json\",\n",
    "    \"technology\": \"technology_news.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6c4ba1-c349-4064-9938-b08ee0d48355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated json with 25 new articles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'primary_article': {'headline': 'After important gains at COP30, India tells developed world: Don‚Äôt expect us to fill in for your failures',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/india/india-cop30-developed-world-climate-change-10381582/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/COP30.jpg?w=450',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025 14:06 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'BJP lashes out after Jamiat Ulama-i-Hind President Arshad Madani claims discrimination of Muslims in India',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/india/arshad-madani-muslims-discrimination-india-congress-bjp-react-10381448/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/Arshad-Madani-.jpg?w=450',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  13:30 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Cyclone Senyar: Landfall timing, path, possible impact, here‚Äôs all you need to know',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/india/cyclone-senyar-landfall-timing-path-possible-impact-heres-all-you-need-to-know-10381211/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/10/30_35b7af.jpg?w=450',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  14:56 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "index = \"india\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "for art in soup.find_all(\"div\", class_=\"articles\"):\n",
    "    try:\n",
    "        img_tag = art.find(\"div\", class_=\"snaps\").find(\"img\") if art.find(\"div\", class_=\"snaps\") else None\n",
    "        img_url = (img_tag.get(\"data-src\") or img_tag.get(\"src\")) if img_tag else \"N/A\"\n",
    "\n",
    "        text = art.find(\"div\", class_=\"img-context\")\n",
    "        if not text: continue\n",
    "\n",
    "        a = text.find(\"h2\", class_=\"title\").find(\"a\") if text.find(\"h2\", class_=\"title\") else None\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a else \"N/A\"\n",
    "        date = text.find(\"div\", class_=\"date\").text.strip() if text.find(\"div\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE india_news.json HERE\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Load old data (if file exists)\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []   # file didn't exist ‚Üí start fresh\n",
    "\n",
    "# Append new articles\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Remove duplicates by article_link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "# Save back to the same file\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Updated json with\", len(final_output), \"new articles.\")\n",
    "final_output[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b8e42e-34df-4b9c-8ccf-bf3e2f1efefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/section/entertainment/?ref=l1_section\n",
      "entertainment_news.json\n",
      "Updated json with 25 new articles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'primary_article': {'headline': 'Netra Mantena‚ÄìVamsi Gadiraju wedding photos out: Donald Trump Jr among guests; Jennifer Lopez seen rehearsing',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/entertainment/bollywood/netra-mantena-vamsi-gadiraju-wedding-photos-out-donald-trump-jr-among-guests-jennifer-lopez-seen-rehearsing-10381673/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/Netra-Mantena-and-Vamsi-Gadirajus-grand-wedding-ll.jpg?w=270',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  15:18 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Rajinikanth, Nani, Jackie Shroff, Brahmanandam attend event celebrating 50 years of Mohan Babu. See photos, videos',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/entertainment/telugu/rajinikanth-nani-jackie-shroff-brahmanandam-attend-event-celebrating-50-years-of-mohan-babu-see-photos-videos-10381451/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/Mohan-Babus-50-years-celebration.jpg?w=270',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  14:15 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'After Ranveer Singh‚Äôs performance at Udaipur wedding, old clip resurfaces of Ranbir Kapoor looking down on dancing at weddings: ‚ÄòI don‚Äôt want to lose my dignity‚Äô',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/entertainment/bollywood/when-ranbir-kapoor-looked-down-on-dancing-at-weddings-i-dont-want-to-lose-my-dignity-10381332/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/MixCollage-23-Nov-2025-12-35-PM-4448.jpg?w=270',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025 13:34 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "index = \"entertainment\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "print(url)\n",
    "print(json_path)\n",
    "\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "# NEW selector for entertainment\n",
    "for art in soup.find_all(\"article\", class_=\"myie-articles\"):\n",
    "    try:\n",
    "        img_wrap = art.find(\"div\", class_=\"myie-snaps\")\n",
    "        img_tag = img_wrap.find(\"img\") if img_wrap else None\n",
    "        img_url = img_tag.get(\"data-src\") or img_tag.get(\"src\") if img_tag else \"N/A\"\n",
    "\n",
    "        title_div = art.find(\"h2\", class_=\"myie-title\")\n",
    "        a = title_div.find(\"a\") if title_div else None\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a else \"N/A\"\n",
    "\n",
    "        time_div = art.find(\"div\", class_=\"my-time\")\n",
    "        date = time_div.text.strip() if time_div else \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        continue\n",
    "\n",
    "# -----------------------------\n",
    "# UPDATE entertainment JSON\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Updated json with\", len(final_output), \"new articles.\")\n",
    "final_output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b747316d-4c7d-4bd8-a763-9dd64e9fc1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/politics/?ref=l1_section\n",
      "‚ùå Could not find opinion-more-listing flex\n",
      "‚úÖ Found 0 articles\n",
      "‚úÖ Updated json with 0 new politics articles.\n",
      "üìä Total unique articles: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "index = \"politics\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "print(url)\n",
    "\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "# CORRECTED SELECTOR: Find the main wrapper first\n",
    "wrapper = soup.find(\"div\", class_=\"opinion-more-wrapper\")\n",
    "\n",
    "if wrapper:\n",
    "    # Find the listing container\n",
    "    listing = wrapper.find(\"div\", class_=\"opinion-more-listing flex\")\n",
    "    \n",
    "    if listing:\n",
    "        # Now find all articles within the listing\n",
    "        for art in listing.find_all(\"div\", class_=\"o-opin-article\"):\n",
    "            try:\n",
    "                # Author + Date\n",
    "                writer = art.find(\"div\", class_=\"news-writer-name\")\n",
    "                author = \"N/A\"\n",
    "                date = \"N/A\"\n",
    "                \n",
    "                if writer:\n",
    "                    author_link = writer.find(\"a\")\n",
    "                    if author_link:\n",
    "                        author = author_link.text.strip()\n",
    "                    \n",
    "                    date_span = writer.find(\"span\", class_=\"opinion-date\")\n",
    "                    if date_span:\n",
    "                        date = date_span.text.strip()\n",
    "                \n",
    "                # Headline + link (from h4 inside the article)\n",
    "                title = art.find(\"h4\", class_=\"o-opin-article_title\")\n",
    "                headline = \"N/A\"\n",
    "                link = \"N/A\"\n",
    "                \n",
    "                if title:\n",
    "                    a = title.find(\"a\")\n",
    "                    if a:\n",
    "                        headline = a.text.strip()\n",
    "                        link = a.get(\"href\", \"N/A\")\n",
    "                        # Make sure link is absolute\n",
    "                        if link != \"N/A\" and not link.startswith(\"http\"):\n",
    "                            link = \"https://indianexpress.com\" + link\n",
    "                \n",
    "                # Summary text\n",
    "                summary_div = art.find(\"div\", class_=\"opinion-news-para\")\n",
    "                summary = summary_div.text.strip() if summary_div else \"N/A\"\n",
    "                \n",
    "                # Image\n",
    "                img_div = art.find(\"div\", class_=\"opinion-news-figure\")\n",
    "                img_url = \"N/A\"\n",
    "                \n",
    "                if img_div:\n",
    "                    # First try to find 'a' tag with img inside\n",
    "                    img_link = img_div.find(\"a\")\n",
    "                    if img_link:\n",
    "                        img_tag = img_link.find(\"img\")\n",
    "                    else:\n",
    "                        img_tag = img_div.find(\"img\")\n",
    "                    \n",
    "                    if img_tag:\n",
    "                        img_url = img_tag.get(\"data-src\") or img_tag.get(\"src\") or \"N/A\"\n",
    "                \n",
    "                # Only add if we have at least a headline\n",
    "                if headline != \"N/A\":\n",
    "                    final_output.append({\n",
    "                        \"primary_article\": {\n",
    "                            \"headline\": headline,\n",
    "                            \"author\": author,\n",
    "                            \"article_link\": link,\n",
    "                            \"featured_image\": img_url,\n",
    "                            \"source_logo\": \"N/A\",\n",
    "                            \"source_name\": \"IndianExpress\",\n",
    "                            \"publish_date\": date,\n",
    "                            \"summary\": summary\n",
    "                        },\n",
    "                        \"related_articles\": [],\n",
    "                        \"total_related_articles\": 0\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"Error parsing article:\", e)\n",
    "                continue\n",
    "    else:\n",
    "        print(\"‚ùå Could not find opinion-more-listing flex\")\n",
    "else:\n",
    "    print(\"‚ùå Could not find opinion-more-wrapper\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(final_output)} articles\")\n",
    "\n",
    "# -----------------------------\n",
    "# UPDATE politics JSON\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by link\n",
    "unique = []\n",
    "links = set()\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links and link != \"N/A\":\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Updated json with {len(final_output)} new politics articles.\")\n",
    "print(f\"üìä Total unique articles: {len(unique)}\")\n",
    "\n",
    "# Display first 3 articles\n",
    "final_output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8627aac-e7e0-49e4-9bb7-5870c17cdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/section/sports/?ref=l1_section\n",
      "Updated json with 25 new articles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'primary_article': {'headline': 'Senuran Muthusamy ‚Äì ‚Äòorigins in India‚Äô, but ‚Äòfirmly South African‚Äô',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/sports/cricket/senuran-muthusamy-origins-in-india-but-firmly-south-african-10381724/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/Senuran-Muthusamy-AP-Photo-2.jpg?resize=450,253',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025 16:10 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': '‚ÄòThere is one area we need to improve, which is our catching‚Äô: Aakash Chopra',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/sports/cricket/there-is-one-area-we-need-to-improve-which-is-our-catching-aakash-chopra-10381622/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/rahul_92e8ba.jpg?resize=450,253',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  14:36 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Lakshya Sen: ‚ÄòCould stay calm in closing stages of first game‚Äô',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/article/sports/badminton/lakshya-sen-could-stay-calm-in-closing-stages-offirst-game-10381536/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/lakshya-sen-3.jpg?resize=450,253',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'November 23, 2025  13:49 IST',\n",
       "   'summary': 'N/A'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "index = \"sports\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "for art in soup.find_all(\"div\", class_=\"articles\"):\n",
    "    try:\n",
    "        img_tag = art.find(\"div\", class_=\"snaps\").find(\"img\") if art.find(\"div\", class_=\"snaps\") else None\n",
    "        img_url = (img_tag.get(\"data-src\") or img_tag.get(\"src\")) if img_tag else \"N/A\"\n",
    "\n",
    "        text = art.find(\"div\", class_=\"img-context\")\n",
    "        if not text: continue\n",
    "\n",
    "        a = text.find(\"h2\", class_=\"title\").find(\"a\") if text.find(\"h2\", class_=\"title\") else None\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a else \"N/A\"\n",
    "        date = text.find(\"div\", class_=\"date\").text.strip() if text.find(\"div\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE india_news.json HERE\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Load old data (if file exists)\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []   # file didn't exist ‚Üí start fresh\n",
    "\n",
    "# Append new articles\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Remove duplicates by article_link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "# Save back to the same file\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Updated json with\", len(final_output), \"new articles.\")\n",
    "final_output[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b06973ab-c37c-4b58-8e0a-62b5f790fa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/section/world/?ref=l1_section\n",
      "Updated json with 0 new articles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "index = \"world\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "print(url)\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "for art in soup.find_all(\"div\", class_=\"articles\"):\n",
    "    try:\n",
    "        img_tag = art.find(\"div\", class_=\"snaps\").find(\"img\") if art.find(\"div\", class_=\"snaps\") else None\n",
    "        img_url = (img_tag.get(\"data-src\") or img_tag.get(\"src\")) if img_tag else \"N/A\"\n",
    "\n",
    "        text = art.find(\"div\", class_=\"img-context\")\n",
    "        if not text: continue\n",
    "\n",
    "        a = text.find(\"h2\", class_=\"title\").find(\"a\") if text.find(\"h2\", class_=\"title\") else None\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a else \"N/A\"\n",
    "        date = text.find(\"div\", class_=\"date\").text.strip() if text.find(\"div\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE india_news.json HERE\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Load old data (if file exists)\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []   # file didn't exist ‚Üí start fresh\n",
    "\n",
    "# Append new articles\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Remove duplicates by article_link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "# Save back to the same file\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Updated json with\", len(final_output), \"new articles.\")\n",
    "final_output[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47865c9b-5453-416e-ac65-e178a763ac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/section/business/?ref=l1_section\n",
      "Updated json with 25 new business articles.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'primary_article': {'headline': 'India Inc pins hopes on implementation of labour codes by states for effective transition',\n",
       "   'author': 'Ravi Dutta Mishra',\n",
       "   'article_link': 'https://indianexpress.com/article/business/india-inc-pins-hopes-on-implementation-of-labour-codes-by-states-for-effective-transition-10379196/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/EMPL14id.jpeg?w=300',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 21, 2025',\n",
       "   'summary': \"India's largest IT industry association Nasscom said the government‚Äôs notification bringing key provisions of the labour codes into effect marks a significant moment in India‚Äôs labour reform journey.\"},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Centre may widen ‚Äòobscene‚Äô content net to ‚Äòhalf truths,‚Äô criticism of social, public figures',\n",
       "   'author': 'Soumyarendra Barik',\n",
       "   'article_link': 'https://indianexpress.com/article/business/centre-obscene-content-net-half-truths-criticism-social-public-figures-10379027/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/netflix-file-photo.jpg?w=300',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 21, 2025',\n",
       "   'summary': 'These changes are being considered for part III of the IT Rules, which covers over-the-top (OTT) video platforms like Netflix, Prime Video and Disney+ Hotstar and digital news publications.'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Rupee falls to record low of 89.41: Here‚Äôs why',\n",
       "   'author': 'Hitesh Vyas',\n",
       "   'article_link': 'https://indianexpress.com/article/business/rupee-falls-to-record-low-of-89-41-heres-why-10378798/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/rupee_7e42de.jpg?w=300',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 21, 2025',\n",
       "   'summary': 'The rupee plunged 70 paise, or 0.79 per cent, to close at 89.41, marking the biggest single-day fall since May 8, compared to the previous close of 88.71.'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "index = \"business\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "print(url)\n",
    "\n",
    "# Load all headers from CSV\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "# BUSINESS: all articles inside <div class=\"o-opin-article\">\n",
    "for art in soup.find_all(\"div\", class_=\"o-opin-article\"):\n",
    "    try:\n",
    "        # AUTHOR + DATE\n",
    "        writer = art.select_one(\"div.news-writer-name\")\n",
    "        author = writer.select_one(\"a\").text.strip() if writer and writer.select_one(\"a\") else \"N/A\"\n",
    "        date = writer.select_one(\"span.opinion-date\").text.strip() if writer and writer.select_one(\"span.opinion-date\") else \"N/A\"\n",
    "\n",
    "        # HEADLINE (CSS SELECTOR FIX)\n",
    "        a = art.select_one(\"a.opinion-news-title\")\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a else \"N/A\"\n",
    "\n",
    "        # SUMMARY\n",
    "        summary_block = art.select_one(\"div.opinion-news-para\")\n",
    "        summary = summary_block.text.strip() if summary_block else \"N/A\"\n",
    "\n",
    "        # IMAGE\n",
    "        img_tag = art.select_one(\"div.opinion-news-figure img\")\n",
    "        img_url = img_tag.get(\"data-src\") or img_tag.get(\"src\") if img_tag else \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": author,\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": summary\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# UPDATE business JSON\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# REMOVE DUPLICATES\n",
    "unique = []\n",
    "seen = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in seen:\n",
    "        seen.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Updated json with\", len(final_output), \"new business articles.\")\n",
    "final_output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "341cb11a-a89f-47f3-a2e0-4080d69bb6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/section/technology/?ref=l1_section\n",
      "Updated technology_news.json with 25 new articles.\n",
      "Total unique articles: 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'primary_article': {'headline': 'Gemini 3 vs Gemini 3 Pro vs Gemini 3 DeepThink: A quick guide to Google‚Äôs latest AI models',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/section/technology/artificial-intelligence/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/11/Tech-feature-images159.jpg?w=320',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 23, 2025',\n",
       "   'summary': 'N/A',\n",
       "   'category': 'Artificial Intelligence'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'Google targets 1000x compute growth, aims to double capacity every 6 months: Report',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/section/technology/artificial-intelligence/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/05/Sundar-Pichai.jpg?w=320',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 23, 2025',\n",
       "   'summary': 'N/A',\n",
       "   'category': 'Artificial Intelligence'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0},\n",
       " {'primary_article': {'headline': 'The fate of Google‚Äôs ad tech monopoly is now in a judge‚Äôs hands',\n",
       "   'author': 'N/A',\n",
       "   'article_link': 'https://indianexpress.com/section/technology/tech-news-technology/',\n",
       "   'featured_image': 'https://images.indianexpress.com/2025/05/Google-Reuters.jpg?w=320',\n",
       "   'source_logo': 'N/A',\n",
       "   'source_name': 'IndianExpress',\n",
       "   'publish_date': 'Nov 23, 2025',\n",
       "   'summary': 'N/A',\n",
       "   'category': 'Tech'},\n",
       "  'related_articles': [],\n",
       "  'total_related_articles': 0}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "index = \"technology\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "print(url)\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "\n",
    "# MAIN: all technology news rows\n",
    "for art in soup.find_all(\"div\", class_=\"area-row\"):\n",
    "\n",
    "    try:\n",
    "        # -------------------------------\n",
    "        # IMAGE\n",
    "        # -------------------------------\n",
    "        img_tag = art.select_one(\"figure.thumb-wrap img\")\n",
    "        img_url = (\n",
    "            img_tag.get(\"data-src\") or \n",
    "            img_tag.get(\"src\") \n",
    "        ) if img_tag else \"N/A\"\n",
    "\n",
    "        # -------------------------------\n",
    "        # CATEGORY\n",
    "        # -------------------------------\n",
    "        category_tag = art.select_one(\"span.category\")\n",
    "        category = category_tag.text.strip() if category_tag else \"N/A\"\n",
    "\n",
    "        # -------------------------------\n",
    "        # DATE\n",
    "        # -------------------------------\n",
    "        date_tag = art.select_one(\"time.time-stamp\")\n",
    "        date = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "\n",
    "        # -------------------------------\n",
    "        # HEADLINE\n",
    "        # -------------------------------\n",
    "        h = art.select_one(\"h3.list-heading\")\n",
    "        headline = h.text.strip() if h else \"N/A\"\n",
    "\n",
    "        # -------------------------------\n",
    "        # LINK\n",
    "        # The <a> wrapping the whole content area\n",
    "        # -------------------------------\n",
    "        a_tag = art.find(\"a\", href=True)\n",
    "        link = a_tag[\"href\"] if a_tag else \"N/A\"\n",
    "\n",
    "        # -------------------------------\n",
    "        # SUMMARY\n",
    "        # Technology section DOES NOT have summary text in list view\n",
    "        # -------------------------------\n",
    "        summary = \"N/A\"\n",
    "\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"IndianExpress\",\n",
    "                \"publish_date\": date,\n",
    "                \"summary\": summary,\n",
    "                \"category\": category\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        continue\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# UPDATE JSON FILE\n",
    "# -------------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "unique = []\n",
    "seen = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in seen:\n",
    "        seen.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Updated {json_path} with {len(final_output)} new articles.\")\n",
    "print(f\"Total unique articles: {len(unique)}\")\n",
    "\n",
    "final_output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e5f5b-1b72-4f55-a44f-533c395eef59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
