{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd379704-8e0c-421e-bcbf-2fa3c4af57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_URLS = {\n",
    "    \"india\": \"https://www.thehindu.com/news/national/\",\n",
    "    \"entertainment\": \"https://www.thehindu.com/entertainment/movies/\",\n",
    "    \"politics\": \"https://www.thehindu.com/elections/\",\n",
    "    \"sports\": \"https://www.thehindu.com/sport/cricket/\",\n",
    "    \"world\": \"https://www.thehindu.com/news/international/\",\n",
    "    \"business\": \"https://www.thehindu.com/business/\",\n",
    "    \"science\": \"https://www.thehindu.com/sci-tech/science/\",\n",
    "    \"technology\": \"https://www.thehindu.com/sci-tech/technology/\",\n",
    "    \"health\": \"https://www.thehindu.com/sci-tech/health/\",\n",
    "    \"real_estate\": \"https://www.thehindu.com/real-estate/\",\n",
    "    \"agriculture\": \"https://www.thehindu.com/sci-tech/agriculture/\",\n",
    "    \"cities\": \"https://www.thehindu.com/news/cities/\",\n",
    "    \"internet\": \"https://www.thehindu.com/sci-tech/technology/internet/\"\n",
    "}\n",
    "JSON_NAMES = {\n",
    "    \"india\": \"india_news.json\",\n",
    "    \"entertainment\": \"entertainment_news.json\",\n",
    "    \"politics\": \"politics_news.json\", \n",
    "    \"sports\": \"sports_news.json\",\n",
    "    \"world\": \"world_news.json\",\n",
    "    \"business\": \"business_news.json\",\n",
    "    \"technology\": \"technology_news.json\",\n",
    "    \"health\":\"health_news.json\",\n",
    "    \"science\":\"science_news.json\",\n",
    "    \"real_estate\":\"local_news.json\",\n",
    "    \"agriculture\": \"india_news.json\",\n",
    "    \"cities\": \"india_news.json\",\n",
    "    \"internet\": \"technology_news.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b94a9f1a-27e4-46dd-a2f9-d53b9487f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"internet\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea177d64-98c3-4eb2-8e44-5c0b9493e1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"cities\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097394f-a3a1-4375-b1cd-87540c337644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"agriculture\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b739abbb-e82b-4de5-832f-803bbb675e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 14 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"real_estate\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a02412-6173-4e46-99a0-760877838b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"india\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7fe0cca-76f0-4459-b8c3-166e6ac36772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 13 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"entertainment\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58c57323-a30c-4535-a2df-72d41d301cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"sports\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77b5401-df7b-4cdd-9587-a60b2ac3c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"world\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9d78d30-a30a-41fd-b942-e18916bf6704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"business\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2741b4f8-f0cd-442a-9612-b288bd68e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 11 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"technology\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1469056-51ce-4c05-a689-738ad4008447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"health\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2caf370-2e61-4743-acbc-3d0879a87ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 11 new articles from The Hindu.\n"
     ]
    }
   ],
   "source": [
    "import json, csv, random, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "index = \"science\"\n",
    "url = CATEGORY_URLS[index]\n",
    "json_path = JSON_NAMES[index]\n",
    "\n",
    "# Load headers\n",
    "header_list = []\n",
    "with open(\"headers.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        header_list.append({\n",
    "            \"User-Agent\": row[\"User-Agent\"],\n",
    "            \"Accept-Language\": row[\"Accept-Language\"]\n",
    "        })\n",
    "\n",
    "def get_random_header():\n",
    "    return random.choice(header_list)\n",
    "\n",
    "response = requests.get(url, headers=get_random_header())\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "final_output = []\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# LOOP THROUGH ALL ARTICLES\n",
    "for art in soup.find_all(\"div\", class_=\"element row-element\"):\n",
    "    try:\n",
    "        # --- IMAGE ---\n",
    "        img_tag = art.find(\"div\", class_=\"picture\")\n",
    "        if img_tag:\n",
    "            img = img_tag.find(\"img\")\n",
    "            img_url = img.get(\"data-original\") or img.get(\"src\") if img else \"N/A\"\n",
    "            #print(img)\n",
    "            #print(img_url)\n",
    "        else:\n",
    "            img_url = \"N/A\"\n",
    "\n",
    "        # --- TEXT CONTENT ---\n",
    "        right = art.find(\"div\", class_=\"right-content\")\n",
    "        if not right:\n",
    "            continue\n",
    "\n",
    "        a = right.find(\"h3\", class_=\"title big\").find(\"a\") if right.find(\"h3\") else None\n",
    "\n",
    "        headline = a.text.strip() if a else \"N/A\"\n",
    "        link = a[\"href\"] if a and a.has_attr(\"href\") else \"N/A\"\n",
    "\n",
    "        # Build object\n",
    "        final_output.append({\n",
    "            \"primary_article\": {\n",
    "                \"headline\": headline,\n",
    "                \"author\": \"N/A\",\n",
    "                \"article_link\": link,\n",
    "                \"featured_image\": img_url,\n",
    "                \"source_logo\": \"N/A\",\n",
    "                \"source_name\": \"The Hindu\",\n",
    "                \"publish_date\": today_date,\n",
    "                \"summary\": \"N/A\"\n",
    "            },\n",
    "            \"related_articles\": [],\n",
    "            \"total_related_articles\": 0\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#  UPDATE JSON FILE\n",
    "# -----------------------------\n",
    "try:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        old_data = json.load(f)\n",
    "except:\n",
    "    old_data = []\n",
    "\n",
    "updated_data = old_data + final_output\n",
    "\n",
    "# Deduplicate by article link\n",
    "unique = []\n",
    "links = set()\n",
    "\n",
    "for article in updated_data:\n",
    "    link = article[\"primary_article\"][\"article_link\"]\n",
    "    if link not in links:\n",
    "        links.add(link)\n",
    "        unique.append(article)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Added\", len(final_output), \"new articles from The Hindu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36087ad6-aaa1-44de-8661-3813611c6c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
