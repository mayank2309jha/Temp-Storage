{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b34454-968b-4de5-b4ae-22e8a65e0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f04ea9-aacc-4707-8144-6f2d622bbacf",
   "metadata": {},
   "source": [
    "## Get random headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4a91a-60a9-4877-adbf-7f2a8002e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We use multiple headers to avoid being blocked by Google News.\"\"\"\n",
    "def fill_headers_list(headers_list):\n",
    "    with open('headers.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            headers = {k: v for k, v in row.items() if v}\n",
    "            headers_list.append(headers)\n",
    "\n",
    "\"\"\"We pick a random header and then proceed. Send HTML request, parse the html into beautiful soup object.\"\"\"\n",
    "def get_soup(url, headers_list):\n",
    "    headers = random.choice(headers_list)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7db85-f9f4-4246-a72b-83b245affb2a",
   "metadata": {},
   "source": [
    "## Get the data from the news cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a07f65-fa9f-4442-b521-f3fd1e4ebaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards_by_headers(soup):\n",
    "    cards = soup.find_all(\"c-wiz\", {\n",
    "        \"jsrenderer\": \"ARwRbe\",\n",
    "        \"jsmodel\": \"hc6Ubd\",\n",
    "        \"class\": \"PO9Zff Ccj79 kUVvS\"\n",
    "    })\n",
    "    return cards\n",
    "def get_news_cards(cards):\n",
    "    return [str(card) for card in cards]\n",
    "\n",
    "\"\"\"Google News builds each news cluster inside a special HTML tag\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff77f83-f52d-4ecb-a5e8-6bd94947b9ca",
   "metadata": {},
   "source": [
    "## Peel the outer html and convert the HTML -> Python Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e1a2e-b431-4f61-b79a-f546bf93c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We peel the outer html to get to the part where we actually have the content.\"\"\"\n",
    "def peel_outer_html(cards):\n",
    "    inner_divs = []\n",
    "    for card in cards:\n",
    "        inner_cwiz = card.find(\"c-wiz\", recursive=True)\n",
    "        if inner_cwiz:\n",
    "            first_div = inner_cwiz.find(\"div\", recursive=False)\n",
    "            if first_div:\n",
    "                second_div = first_div.find(\"div\", recursive=False)\n",
    "                if second_div:\n",
    "                    inner_divs.append(second_div)\n",
    "    return inner_divs\n",
    "\n",
    "\"\"\"Recursively convert a BeautifulSoup element to a nested dictionary structure.\"\"\"\n",
    "\"\"\"\n",
    "Recursively iterates through child elements\n",
    "If tag is simple (p, span, time…) → capture text\n",
    "If tag is a → capture href\n",
    "If img → capture src\n",
    "If tag repeats → stores a list\n",
    "Stores raw text when needed\n",
    "\"\"\"\n",
    "def element_to_dom(element):\n",
    "    dom = {}\n",
    "\n",
    "    for child in element.children:\n",
    "        if child.name is None:\n",
    "            text = child.strip()\n",
    "            if text:\n",
    "                dom[\"text\"] = text\n",
    "            continue\n",
    "\n",
    "        tag = child.name\n",
    "\n",
    "        # Determine value based on tag type\n",
    "        if tag == \"a\":\n",
    "            value = child.get(\"href\")\n",
    "        elif tag == \"img\":\n",
    "            value = child.get(\"src\")\n",
    "        elif tag == \"svg\":\n",
    "            value = str(child)\n",
    "        elif tag in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"span\", \"time\", \"button\"]:\n",
    "            value = child.decode_contents().strip()\n",
    "        else:\n",
    "            # If element has nested tags, recurse\n",
    "            nested = element_to_dom(child)\n",
    "            value = nested if nested else child.decode_contents().strip()\n",
    "\n",
    "        # Handle multiple same-tag children by converting value into a list\n",
    "        if tag in dom:\n",
    "            if isinstance(dom[tag], list):\n",
    "                dom[tag].append(value)\n",
    "            else:\n",
    "                dom[tag] = [dom[tag], value]\n",
    "        else:\n",
    "            dom[tag] = value\n",
    "\n",
    "    return dom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4fa90-9121-43d3-ab6b-9c34f8dbca1e",
   "metadata": {},
   "source": [
    "## Get the news headline, Get primary and related articles for google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb062d-2121-4e57-aa08-fc06b84475d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts structured news data from a single dom_structure[i] element.\n",
    "Returns a dictionary containing:\n",
    "- primary_article (headline, link, author, etc.)\n",
    "- related_articles (list of dictionaries)\n",
    "- total_related_articles (count)\n",
    "\"\"\"\n",
    "def get_news_data(dom_structure, i, inner_divs_html):\n",
    "    BASE_URL = \"https://news.google.com\"\n",
    "    today_date = datetime.now().strftime(\"%d-%m-%Y\") \n",
    "\n",
    "    def get_headline(i):\n",
    "        html = inner_divs_html[i]\n",
    "        temp_soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find the button\n",
    "        button = temp_soup.find('button')\n",
    "        \n",
    "        # Get the aria-label value\n",
    "        if button and button.has_attr('aria-label'):\n",
    "            return button['aria-label']\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    # --- PRIMARY ARTICLE ---\n",
    "    article = dom_structure.get(\"div\", {}).get(\"article\", {})\n",
    "    headline = get_headline(i)\n",
    "    news_data = {\n",
    "        'headline': headline if headline is not None else 'N/A',\n",
    "\n",
    "        'author': article.get('div', [])[2].get('div', {}).get('span', 'N/A')\n",
    "        if isinstance(article.get('div', []), list) and len(article.get('div', [])) > 2 else 'N/A',\n",
    "\n",
    "        'article_link': (\n",
    "            BASE_URL + article.get('a', '')\n",
    "            if article.get('a') not in (None, 'N/A') else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'featured_image': article.get('figure', {}).get('img', 'N/A'),\n",
    "\n",
    "        'source_logo': (\n",
    "            article.get('div', [])[1].get('div', [])[0].get('img', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        'source_name': (\n",
    "            article.get('div', [])[1]\n",
    "            .get('div', [])[0]\n",
    "            .get('div', {})\n",
    "            .get('div', {})\n",
    "            .get('text', 'N/A')\n",
    "            if isinstance(article.get('div', []), list)\n",
    "            and len(article.get('div', [])) > 1\n",
    "            and isinstance(article.get('div', [])[1].get('div', []), list)\n",
    "            else 'N/A'\n",
    "        ),\n",
    "\n",
    "        # --- NEW FIELD ---\n",
    "        'publish_date': today_date\n",
    "    }\n",
    "\n",
    "    # --- RELATED ARTICLES ---\n",
    "    related_articles = []\n",
    "    possible_related = dom_structure.get(\"div\", {}).get(\"div\", {}).get(\"article\", [])\n",
    "\n",
    "    if isinstance(possible_related, list):\n",
    "        for article in possible_related:\n",
    "            if not isinstance(article, dict):\n",
    "                continue\n",
    "\n",
    "            divs = article.get('div', [])\n",
    "            if not isinstance(divs, list) or len(divs) < 3:\n",
    "                continue\n",
    "\n",
    "            raw_link = article.get('a', '')\n",
    "            full_link = BASE_URL + raw_link if raw_link not in (None, 'N/A') else 'N/A'\n",
    "\n",
    "            article_data = {\n",
    "                'headline': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'author': divs[2].get('div', {}).get('span', 'N/A'),\n",
    "                'article_link': full_link,\n",
    "                'source_logo': (\n",
    "                    divs[1].get('div', [])[0].get('img', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "                'source_name': (\n",
    "                    divs[1].get('div', [])[0]\n",
    "                    .get('div', {})\n",
    "                    .get('div', {})\n",
    "                    .get('text', 'N/A')\n",
    "                    if len(divs) > 1 and isinstance(divs[1].get('div', []), list)\n",
    "                    else 'N/A'\n",
    "                ),\n",
    "\n",
    "                # --- NEW FIELD ---\n",
    "                'publish_date': today_date\n",
    "            }\n",
    "            related_articles.append(article_data)\n",
    "\n",
    "    # --- FINAL OUTPUT ---\n",
    "    complete_news_data = {\n",
    "        'primary_article': news_data,\n",
    "        'related_articles': related_articles,\n",
    "        'total_related_articles': len(related_articles)\n",
    "    }\n",
    "\n",
    "    return complete_news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f82640-2866-43b9-b0d5-151d5c3f0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, file):\n",
    "    headers_list = []\n",
    "    fill_headers_list(headers_list)\n",
    "    soup = get_soup(url, headers_list)\n",
    "    cards = get_cards_by_headers(soup)\n",
    "    news_cards = get_news_cards(cards)\n",
    "    inner_divs = peel_outer_html(cards)\n",
    "    inner_divs_html = [str(div) for div in inner_divs]\n",
    "    dom_structures = []\n",
    "    for html in inner_divs_html:\n",
    "        temp_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        dom_dict = element_to_dom(temp_soup)\n",
    "        dom_structures.append(dom_dict)\n",
    "    news_data = []\n",
    "\n",
    "    for i, dom in enumerate(dom_structures):\n",
    "        try:\n",
    "            news_item = get_news_data(dom, i, inner_divs_html)\n",
    "            news_data.append(news_item)\n",
    "            # print(f\"Processed item {i + 1}/{len(dom_structures)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i + 1}: {e}\")\n",
    "    \n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(news_data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\n✅ File \"+file+\" saved successfully in the working directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed055442-bed0-4c6c-8e2d-7392f213fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "science = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp0Y1RjU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(science, 'science_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578136c7-06ab-4b8d-87e2-cbb374ad4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(world, 'world_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab6041-0372-4bee-ab43-b824be282edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "technology = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(technology, 'technology_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb0c22-2aa3-429b-b9e7-8db1765a075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(sports, 'sports_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34041ab-fa6b-4b7e-a5ff-7de7a037e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local = \"https://news.google.com/topics/CAAqHAgKIhZDQklTQ2pvSWJHOWpZV3hmZGpJb0FBUAE/sections/CAQiTkNCSVNORG9JYkc5allXeGZkakpDRUd4dlkyRnNYM1l5WDNObFkzUnBiMjV5Q2hJSUwyMHZNR1JzZGpCNkNnb0lMMjB2TUdSc2RqQW9BQSowCAAqLAgKIiZDQklTRmpvSWJHOWpZV3hmZGpKNkNnb0lMMjB2TUdSc2RqQW9BQVABUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(local, 'local_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab013ec-34eb-4f67-b817-029e8db7cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "india = \"https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNRE55YXpBU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(india, \"india_news.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550ca92-f94f-49a2-96f6-e2084eee3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "health = \"https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JXVnVMVWRDS0FBUAE?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(health, 'health_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444417ca-f875-48cb-96dc-55bd8b0bb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(entertainment, 'entertainment_news.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fac410-cab8-46b5-b9c6-8029b58b2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "business = \"https://news.google.com/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0JXVnVMVWRDR2dKSlRpZ0FQAQ?hl=en-IN&gl=IN&ceid=IN%3Aen\"\n",
    "\n",
    "scrape(business, 'business_news.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
